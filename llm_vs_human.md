我们可以探究大模型的哪些能力？
* 知识广度和信息整合——这点条过
* 推理与决策——现在一直在攻关的事情,Chain of Thought等技术的提出
* 语言理解与生成——理解专指一些复杂的语言情境、隐喻、幽默等方面
* 情感识别与共情——情感识别可能已经有不错的进展，但是回应情感需求呢？
* 适应性和自我改进、自我学习——能否像人类一样灵活适应新环境？“自主学习”新知识和策略？
* 伦理和价值判断——LLM在应用于敏感场景的局限性

* LLM是否具有`心智`？
* LLM是否具有拟人的情感？
* LLM是否具有记忆能力，推理能力，情感感知能力

从这一篇“nature human behavior”出发
### Testing `theory of mind` in large language models and humans
在LLM和人类中检测心理理论
- [./papers/s41562-024-01882-z](./papers/s41562-024-01882-z.pdf)
#### 摘要提取
1. **研究主题**：探索大语言模型（LLMs），如ChatGPT和LLaMA2，在心智理论(Theory of Mind)任务中的表现，研究其与人类在理解他人心理状态上的可比性。
2. **测试内容**：研究中使用了两个不同的LLM模型(GPT-4和LLaMA2)进行了一系列理论推理测验并和人类参与者做比较，研究选择了四个理论心理学测试：
    * 错误观念
    * 间接请求
    * 讽刺
    * 社交失误(faus pas)
3. **研究结果**：
   - GPT-4在间接请求、错误信念和误导理解方面的表现与人类相当，甚至在某些方面优于人类。
   - LLaMA2在辨别社交失误的测试中优于人类，但这种优势可能是由于其偏向于将情境解读为无知而产生的假象。
   - GPT模型在推理时较为保守，表现出一种“超保守”倾向，导致其在一些推理任务中不愿轻易得出结论。
4. **研究意义**：研究表明，LLMs的推理输出与人类推理相似，但要确保两者的比较具有实质性，需要进行系统化的测试和深入分析。

#### 一句话
这篇是在探索现有的大模型在做判断的时候(思维理论方面)“像不像人”，结果是一些方面差不多，但是一些方面因为模型本身的特质，比如“GPT-4推理时超保守”，“LLaMA2可能偏向将情景解读为无知而在辨别社交失误的测试中优于人类”？得到这些模型比较像人，但是有偏？